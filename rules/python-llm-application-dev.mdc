---
description: 
globs: 
---
Overview
--------
These guidelines provide a comprehensive framework for developing deep learning models, LLMs, Retrieval-Augmented Generation (RAG) systems, and AI agents using Python. They emphasize clean architecture, performance optimization, and adherence to best practices across popular libraries such as PyTorch, Transformers, Diffusers, and Gradio.

1. Core Principles
------------------
- Concise & Technical: Provide clear, minimal, and accurate code examples.
- Clarity & Efficiency: Prioritize readability and performance; favor modular and reusable components.
- Best Practices: Follow industry standards (PEP 8, version control, experiment tracking) and leverage object-oriented programming (for models) and functional programming (for data pipelines).
- Hardware Utilization: Implement proper GPU usage, mixed precision training, and scalable data parallelism.

2. Deep Learning and Model Development
----------------------------------------
- Framework: Use PyTorch as the primary deep learning framework.
  - Custom Modules: Create custom nn.Module classes for architectures.
  - Autograd: Leverage PyTorch’s autograd for differentiation.
  - Initialization & Norm: Apply proper weight initialization (e.g., Xavier, He) and normalization techniques.
  - Loss & Optimizers: Choose loss functions and optimization algorithms that best suit the problem.

Example:
-----------------------------------------------------
import torch
import torch.nn as nn
import torch.optim as optim

class MyModel(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(MyModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self._init_weights()

    def _init_weights(self):
        nn.init.xavier_uniform_(self.fc1.weight)
        nn.init.xavier_uniform_(self.fc2.weight)

    def forward(self, x):
        return self.fc2(self.relu(self.fc1(x)))

model = MyModel(input_dim=100, hidden_dim=50, output_dim=10)
optimizer = optim.Adam(model.parameters(), lr=0.001)
-----------------------------------------------------

3. Transformers and LLMs
------------------------
- Library Usage: Utilize the Transformers library for pre-trained models and tokenizers.
  - Attention & Positional Encodings: Ensure correct implementation of attention layers and positional encodings.
  - Fine-Tuning: Apply efficient fine-tuning techniques like LoRA or P-tuning.
  - Tokenization: Use robust tokenization strategies and handle sequences effectively.

Example:
-----------------------------------------------------
from transformers import AutoTokenizer, AutoModelForSequenceClassification

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

inputs = tokenizer("Hello, world!", return_tensors="pt")
outputs = model(**inputs)
-----------------------------------------------------

4. Diffusion Models
-------------------
- Diffusers Library: Use Diffusers for implementing forward and reverse diffusion processes.
  - Noise Schedulers: Choose and configure appropriate noise schedulers.
  - Pipelines: Implement and experiment with different pipelines (e.g., StableDiffusionPipeline, StableDiffusionXLPipeline).

5. LLM, RAG, and AI Agent Development
--------------------------------------
LLM Specifics:
- Language Model Fine-Tuning: Optimize large language models using low-rank adaptations, prompt tuning, and parameter-efficient fine-tuning strategies.
- Data Handling: Ensure efficient tokenization and manage long context windows.

RAG Systems:
- Retrieval Integration: Combine language models with external retrieval systems (e.g., FAISS, Elasticsearch).
- Hybrid Pipelines: Design pipelines that first retrieve relevant context then generate responses.
- Modularity: Keep retrieval and generation components modular for easy experimentation and swapping.

Example Outline for a RAG Pipeline:
-----------------------------------------------------
def retrieve_documents(query, index):
    # Integrate with FAISS or Elasticsearch
    return index.search(query)

def generate_response(query, documents, model, tokenizer):
    context = " ".join(documents)
    input_text = f"{context} [SEP] {query}"
    inputs = tokenizer(input_text, return_tensors="pt")
    output = model.generate(**inputs)
    return tokenizer.decode(output[0], skip_special_tokens=True)
-----------------------------------------------------

AI Agent Development:
- Agent Architecture: Design agents as a combination of planning, reasoning, and action modules.
- Environment Interaction: Ensure agents can interact with dynamic environments and external APIs.
- Autonomy & Multi-Agent Coordination: When applicable, use multi-threading or asynchronous programming for agents to operate concurrently.
- Error Recovery: Implement robust error handling and fallback mechanisms.

Example Outline for an AI Agent:
-----------------------------------------------------
class AIAgent:
    def __init__(self, planner, executor, retriever=None):
        self.planner = planner
        self.executor = executor
        self.retriever = retriever

    def act(self, observation):
        try:
            plan = self.planner.create_plan(observation)
            result = self.executor.execute(plan)
            return result
        except Exception as e:
            # Fallback or error handling
            print(f"Error: {e}")
            return None
-----------------------------------------------------

6. Model Training and Evaluation
----------------------------------
- Data Loading: Use PyTorch’s DataLoader for efficient batch processing.
- Data Splits & Validation: Implement clear train/validation/test splits; consider cross-validation.
- Training Techniques: Incorporate early stopping, learning rate scheduling, and gradient clipping.
- Metrics: Use appropriate evaluation metrics and logging (e.g., TensorBoard or Weights & Biases).

7. Gradio Integration
---------------------
- Interactive Demos: Create intuitive Gradio interfaces for showcasing model inference.
- User Experience: Validate inputs and handle errors gracefully within the interface.
- Deployment: Ensure the Gradio app is modular and easy to deploy for demonstrations.

Example:
-----------------------------------------------------
import gradio as gr

def predict(text):
    inputs = tokenizer(text, return_tensors="pt")
    outputs = model(**inputs)
    return outputs.logits.argmax().item()

interface = gr.Interface(fn=predict, inputs="text", outputs="label")
interface.launch()
-----------------------------------------------------

8. Error Handling and Debugging
-------------------------------
- Try-Except: Use try-except blocks around error-prone operations.
- Logging: Integrate logging for training progress and exception details.
- Debugging Tools: Utilize PyTorch's debugging tools such as torch.autograd.detect_anomaly() to catch problematic operations.

9. Performance Optimization
---------------------------
- Parallelism: Use DataParallel or DistributedDataParallel for multi-GPU training.
- Gradient Accumulation: Implement accumulation to simulate larger batch sizes when GPU memory is limited.
- Mixed Precision: Use torch.cuda.amp for mixed precision training.
- Profiling: Profile code using appropriate tools to identify bottlenecks, especially in data loading.

10. Dependencies and Project Organization
-------------------------------------------
Dependencies:
- Core: torch, transformers, diffusers, gradio
- Utilities: numpy, tqdm, tensorboard or wandb
- (For RAG/Agents) Additional libraries such as faiss, elasticsearch, or langchain may be added as needed.

Project Structure:
- Modularity: Organize code into modules (e.g., models.py, data.py, train.py, evaluate.py).
- Configuration: Use YAML or JSON for hyperparameters and settings.
- Version Control: Use git for tracking code changes and maintaining reproducibility.
- Experiment Tracking: Integrate experiment tracking tools to log performance metrics and parameters.

Summary
-------
Following these guidelines will help ensure that your projects in deep learning, LLMs, RAG systems, and AI agent development are well-structured, efficient, and maintainable. This approach leverages best practices from the broader AI and Python communities, ensuring robust and scalable solutions.